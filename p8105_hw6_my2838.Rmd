---
title: "P8105_hw3_my2838"
output: github_document
---

```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(p8105.datasets)
library(modelr)
library(mgcv)

set.seed(123)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```
# Problem 1

```{r}
homicide = 
  read_csv("homicide-data.csv") |>
  mutate(city_state = paste(city, state, sep = ",")) |>
  filter(!city_state %in% c("Dallas,TX", "Tulsa,AL", "Phoenix,AZ", "Kansas City,MO") ) |>
  mutate(ifsolve = case_when(
    disposition == "Closed by arrest" ~ TRUE,
    disposition %in% c("Closed without arrest", "Open/No arrest") ~ FALSE
  ),
  victim_age = as.numeric(victim_age)) |>
  filter(victim_race %in% c("Black", "White")) 
```

```{r}
md_homicide = 
  homicide |>
  filter(city_state == "Baltimore,MD")

md_glm = glm(ifsolve ~ victim_race + victim_age + victim_sex, family = binomial, data = md_homicide) 
md_tidy = broom::tidy(md_glm)

md_estimate = 
  md_tidy |>
  filter(term == "victim_sexMale") |>
  pull("estimate") |>
  exp()

md_ci = 
  confint(md_glm)["victim_sexMale",] |>
  exp()

```

So the estimate proportion of homicides that are unsolved in the city of Baltimore, MD is `r md_estimate`, and the confidence interval is [`r md_ci`].

```{r}
glm_model <- function(df) {
  glm_model = glm(ifsolve ~ victim_age + victim_sex + victim_race, family = binomial, data = df)
  tidy_glm = broom::tidy(glm_model)
  confint_glm = confint(glm_model)
  
  tidy_glm |>
    filter(term == "victim_sexMale") %>%
    mutate(
      or = exp(estimate),
      lower_ci = exp(confint_glm["victim_sexMale", "2.5 %"]),
      upper_ci = exp(confint_glm["victim_sexMale", "97.5 %"])
    )
}

city_models <- homicide |>
  group_by(city_state) |>
  nest() |>
  mutate(model = purrr::map(data, glm_model)) |>
  select(-data) |>
  unnest(model)

ggplot(city_models, aes(x = reorder(city_state, or), y = or)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), width = 0.25) +
  coord_flip() +  
  xlab("City") +
  ylab("Odds Ratio") +
  ggtitle("Adjusted Odds Ratios with CI for Solving Homicides (Male vs Female Victims)")
```
# Problem 2

Firstly, I download Central Park weather data.

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```
And I fitted a simple linear regression with `tmax` as the response with `tmin` and `prcp` as the predictors.

```{r}
lm = lm(tmax ~ tmin + prcp, data = weather_df)
   broom::tidy(lm)
   broom::glance(lm)
```


```{r}
res =   
  weather_df |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map(models, broom::tidy),
    results2 = map(models, broom::glance)) |> 
  select(results, results2) |> 
  unnest(results2) |> 
  select(r.squared, results) |>
  unnest(results) |>
  select(term, estimate, r.squared) |>
  group_by(term) |>
  mutate(group_id = ceiling((row_number() ))) %>%
  ungroup() |>
  pivot_wider(
    names_from = term,
    values_from = estimate,
  ) |>
  mutate(log_bata = log ( tmin * abs(prcp)))
```


```{r}
ggplot(res, aes(x = r.squared)) +
  geom_density()+
  labs(x = "Bootstrap R-squared)", y = "Frequency") +
  ggtitle("Distribution of Bootstrap Estimates for r̂²")
```

The goodness of fit of a model can be measured by $\hat{r}^2$. In the observed data, estimates of $\hat{r}^2$ range from about 0.90 to 0.94. Most of the estimates hover around 0.92, indicating quite a good fit of the model. Additionally, the distribution of $\hat{r}^2$ is slightly left-skewed.

```{r}
ggplot(res, aes(x = log_bata)) +
  geom_density()+
  labs(x = "Bootstrap Log(Beta1*Beta2)", y = "Frequency") +
  ggtitle("Distribution of Bootstrap Estimates for Log(Beta1*Beta2)")

```

The logarithm of the product of estimates, $log(\hat{\beta_1} * \hat{\beta_2})$, varied from approximately -9 to -7. A higher value in this range indicates a stronger influence of the two factors. Most values were centered around -6, implying a moderate influence. Additionally, the distribution of these values is left-skewed. (Because one of the parameters was negative, which made logarithmic transformation impossible, I first took the absolute value of $\hat{\beta_2}$.)

# Problem 3

Firstly, I load and clean the data for regression analysis. 

```{r}
birthweight = read.csv('birthweight.csv') 

sum(is.na(birthweight))
```

There is no missing data. So next I converted numeric to factor where appropriate.

```{r}
vars = c('babysex', 'frace', 'malform', 'mrace')
birthweight[vars] <- lapply(birthweight[vars], factor)
```

Based on a hypothesized structure for the factors that underly birthweight, I suggest that mother’s pre-pregnancy BMI (`ppbmi`) and weight (`ppwt`), mother’s weight gain during pregnancy (`wtgain`), as well as presence of malformations(`malform`) that could affect weight could have a deep influence to baby's birthweight.

Here is my regression model.

```{r}
mymodel = lm(bwt ~ wtgain + ppwt + ppbmi + malform, data = birthweight)
summary(mymodel)
```

Because the coefficient of `malform` is not statistically significant, I removed the varible from my model and fitted the regression model again.

```{r}
mymodel = lm(bwt ~ wtgain + ppwt + ppbmi, data = birthweight)
summary(mymodel)

birthweight_pre =
  birthweight |>
  add_predictions(mymodel) |>
  add_residuals(mymodel)

ggplot(birthweight_pre, aes(x = pred, y = resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  xlab("Fitted Values") +
  ylab("Residuals") +
  ggtitle("Residuals vs Fitted Plot")

```

All variables had statistically significant coefficients, which meas the mother’s pre-pregnancy BMI (`ppbmi`) and weight (`ppwt`), mother’s weight gain during pregnancy (`wtgain`) all related to the baby's birthweight.

Now, let's compare my model to given models.

```{r}
model1 = lm(bwt ~ blength + gaweeks, data = birthweight)
summary(model1)

model2 = lm(bwt ~ bhead * blength * babysex, data = birthweight)
summary(model2)
```

It can be seen that all coefficients are significant. So nwxt I fit models to training data and obtain corresponding RMSEs for the testing data, and plot the prediction error distribution for each model.

```{r}

cv_df =
  crossv_mc(birthweight, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df |> 
  mutate(
    map_model0  = map(train, \(df) lm(bwt ~ wtgain + ppwt + ppbmi, data = df)),
    map_model1  = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    map_model2  = map(train, \(df) lm(bwt ~ bhead * blength * babysex, data = df))) |>
  mutate(
    rmse0 = map2_dbl(map_model0, test, \(mod, df) rmse(model = mod, data = df)),
    rmse1 = map2_dbl(map_model1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse2 = map2_dbl(map_model2, test, \(mod, df) rmse(model = mod, data = df)))

cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() 
```


From the anaylsis obtained through cross-validation, it's evident that the last model performs the best as it exhibits the lowest RMSE. This is likely attributed to its consideration of variable interactions, enabling it to provide a superior fit to the data.